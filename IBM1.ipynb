{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_train = 'hansards.36.2.e'\n",
    "french_train = 'hansards.36.2.f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IBM1():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.english_training = []\n",
    "        self.french_training = []\n",
    "        self.english_voc = set()\n",
    "        self.french_voc = set()\n",
    "        self.english_indices = dict()\n",
    "        self.english_words = dict()\n",
    "        self.french_indices = dict()\n",
    "        self.french_words = dict()\n",
    "        self.trans_matrix = None\n",
    "        self.likelihoods = []\n",
    "\n",
    "\n",
    "    def read_data(self, english_train, french_train):\n",
    "        print('Start reading data...')\n",
    "\n",
    "        e = open(english_train, 'r', encoding='utf8')\n",
    "        for i, line in enumerate(e):\n",
    "            sentence = line.split()\n",
    "            # add null word to each sentence\n",
    "            sentence = ['NULL'] + sentence\n",
    "            self.english_training.append(sentence)\n",
    "            # add words to vocabulary\n",
    "            self.english_voc.update(sentence)\n",
    "        e.close()\n",
    "\n",
    "        f = open(french_train, 'r', encoding='utf8')\n",
    "        for i, line in enumerate(f):\n",
    "            sentence = line.split()\n",
    "            self.french_training.append(sentence)\n",
    "            self.french_voc.update(sentence)\n",
    "        f.close()\n",
    "\n",
    "        self.map_to_unk(10, self.english_training, self.english_voc)\n",
    "        self.map_to_unk(10, self.french_training, self.french_voc)\n",
    "\n",
    "        for index, e in enumerate(self.english_voc):\n",
    "            self.english_indices[e] = index\n",
    "            self.english_words[index] = e\n",
    "\n",
    "        for index, f in enumerate(self.french_voc):\n",
    "            self.french_indices[f] = index\n",
    "            self.french_words[index] = f\n",
    "        \n",
    "        self.initialize_translation()\n",
    "        \n",
    "        print('Done with reading data!')\n",
    "\n",
    "\n",
    "    def map_to_unk(self, k, training, vocabulary):\n",
    "        counts = Counter(w for sent in training for w in sent)\n",
    "        counted_once = [w for w, count in counts.items() if count == 1]\n",
    "        counted_once = counted_once[0:k]\n",
    "        for i, sentence in enumerate(training):\n",
    "            for j, word in enumerate(sentence):\n",
    "                if training[i][j] in counted_once:\n",
    "                    training[i][j] = 'UNK'\n",
    "        vocabulary.add('UNK')\n",
    "        for word in counted_once:\n",
    "            vocabulary.remove(word)\n",
    "        \n",
    "    def initialize_translation(self):\n",
    "        vocab_dict = defaultdict(list)\n",
    "        for (e,f) in zip(self.english_training, self.french_training):\n",
    "            for word_f in f:\n",
    "                for word_e in e:\n",
    "                    vocab_dict[word_e].append(word_f)\n",
    "        t= defaultdict(Counter)\n",
    "        for word_e in vocab_dict:\n",
    "            words = set(vocab_dict[word_e])\n",
    "            prob = np.ones(len(words)) / len(words)\n",
    "            for i, f in enumerate(words):\n",
    "                t[word_e][f] = prob[i] \n",
    "        self.trans_matrix = t\n",
    "    \n",
    "    \n",
    "    def update_translation(self, count_ef, count_e):\n",
    "        for (eng,fr) in zip(self.english_training, self.french_training):\n",
    "                for f in fr:\n",
    "                    for e in eng:\n",
    "                        self.trans_matrix[e][f] = count_ef[e][f] / count_e[e]\n",
    "    \n",
    " \n",
    "    def run_EM(self):\n",
    "        count_ef = defaultdict(Counter)\n",
    "        count_e = Counter()\n",
    "\n",
    "        for k, (eng, fr) in enumerate(zip(self.english_training, self.french_training)):\n",
    "            for f in fr:\n",
    "                for e in eng:\n",
    "                    delta = self.trans_matrix[e][f] / float(sum([self.trans_matrix[w][f] for w in eng]))\n",
    "                    count_ef[e][f] += delta  \n",
    "                    count_e[e] += delta\n",
    "\n",
    "        self.update_translation(count_ef, count_e)\n",
    "\n",
    "        self.likelihoods.append(self.log_likelihood())\n",
    "\n",
    "        print('Likelihood:', likelihood)\n",
    "        \n",
    "\n",
    "    def log_likelihood(self):\n",
    "        likelihood = 0\n",
    "        for (eng, fr) in zip(self.english_training, self.french_training):\n",
    "            alignment = self.align(fr, eng)\n",
    "            l = 0\n",
    "            for a, f in enumerate(fr):\n",
    "                l += np.log(self.trans_matrix[eng[alignment[a]]][f])\n",
    "            likelihood += l\n",
    "            likelihood += -len(fr) * np.log(len(eng) + 1) \n",
    "\n",
    "        return likelihood\n",
    "                       \n",
    "    def align(self, sent_fr, sent_eng):\n",
    "        alignment = []\n",
    "        for f_i, f in enumerate(sent_fr):\n",
    "            alignment_i = []\n",
    "            highest_prob = -100\n",
    "            for e_i, e in enumerate(sent_eng):\n",
    "                prob = self.trans_matrix[e][f]\n",
    "                if prob > highest_prob:\n",
    "                    highest_prob = prob\n",
    "                    alignment_i = e_i\n",
    "            alignment.append(alignment_i)\n",
    "        return alignment\n",
    "\n",
    "    def predict_alignment(self, test_fr, test_eng, outpath):\n",
    "        f_test = open(test_fr, 'r')\n",
    "        e_test = open(test_eng, 'r')\n",
    "        f_sents = []\n",
    "        e_sents = []\n",
    "        for line in f_testfile:\n",
    "            f_sents.append(line.split())\n",
    "        for line in e_testfile:\n",
    "            e_sents.append(['NULL'] + line.split())\n",
    "\n",
    "        alignments = []\n",
    "\n",
    "        for F, E in zip(f_sents, e_sents):\n",
    "            alignment = self.align(F, E)\n",
    "            alignments.append(alignment)\n",
    "        f_testfile.close()\n",
    "        e_testfile.close()\n",
    "\n",
    "        output = open(outpath, 'w')\n",
    "        nulls = 0\n",
    "        for k, alignment in enumerate(alignments):\n",
    "            for f, e in enumerate(alignment):\n",
    "                if e != 0:\n",
    "                    output.write('{0} {1} {2} {3}\\n'.format(k + 1, e, f + 1, 'S'))\n",
    "\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading data...\n",
      "Done with reading data!\n"
     ]
    }
   ],
   "source": [
    "ibm = IBM1()\n",
    "ibm.read_data(english_train, french_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'likelihood' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-46fde93d585b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mibm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_EM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mibm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_alignment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dev.f'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'dev.e'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/prediction/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'result-{0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-4cd59dd315cb>\u001b[0m in \u001b[0;36mrun_EM\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihoods\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Likelihood:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'likelihood' is not defined"
     ]
    }
   ],
   "source": [
    "steps = 2\n",
    "\n",
    "for k in range(steps):\n",
    "    ibm.run_EM()\n",
    "    ibm.predict_alignment('dev.f','dev.e', '/prediction/' + 'result-{0}'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elbo(likelihood, lmbda_fe, alpha):\n",
    "    KL = (np.sum(np.multiply(self.t, -1*lmbda_fe + alpha) + loggamma(lmbda_fe), axis=0, keepdims=True) - \n",
    "            self.V_f_size * loggamma(alpha) +\n",
    "            loggamma(self.V_f_size * alpha) -\n",
    "            loggamma(np.sum(lmbda_fe, axis=0, keepdims=True)))\n",
    "    return likelihood + (np.sum(KL, axis=1)[0]).real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Likelihood\n",
    "plt.plot(range(len(likelihoods)), likelihoods)\n",
    "\n",
    "\n",
    "\n",
    "# ELBO\n",
    "plt.plot(range(len(self.elbos)), self.elbos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
